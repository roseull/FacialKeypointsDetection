{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    X.shape: (2140, 96, 96, 1)   2140 fotos of size 96x96, 1 channel\n",
    "    label.shape: (2140, 30)      each foto has 15 key_points, each key_point has (x, y)\n",
    "'''\n",
    "def load_training_data(training_file):    \n",
    "    df = pd.read_csv(training_file)\n",
    "    \n",
    "    # columns[:-1] : label y(s);\n",
    "    # the last colume is raw 'image'. exclude it.\n",
    "    label_cols = df.columns[:-1] \n",
    "    print('label_cols (no image):', label_cols)\n",
    "    \n",
    "    # drop all incompleted samples. only 2140 completed samples are left.\n",
    "    # *** should do some feature engineering here to make up those incompleted samples ***\n",
    "    df = df.dropna()\n",
    "#    print(df.head())\n",
    "    \n",
    "    # df['Image'] is the last column (input x)\n",
    "    # stratch the pixel values of image(input x) to [0, 1]\n",
    "    df['Image'] = df['Image'].apply(lambda pixel: np.fromstring(pixel, sep=' ') / 255.0)\n",
    "    \n",
    "    # --------- reshape the raw image from 9216 to 96x96x1 ---------\n",
    "    #   2140 valid fotos in total. each foto has 9216 pixels.\n",
    "    #   df[Image].shape: (2140,),   df[Image][0].shape: (9216,)\n",
    "    #   transfer it into ndarray of shape(2140, 9216) , then reshape to (2140, 96, 96, 1)\n",
    "    X = np.vstack(df['Image'])      \n",
    "    X = X.reshape((-1, 96, 96, 1)) \n",
    "\n",
    "    # labels y(s), which are the all columns but the last one\n",
    "    # y.shape = (2140, 30)   15 keypoints each face with x, y for each keypoint\n",
    "    labels = df[label_cols].values / 96.0\n",
    "    print('labels.shape: ', labels.shape) \n",
    "\n",
    "    # X.shape: (2140, 96, 96, 1)\n",
    "    # y.shape: (2140, 30)\n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, labels = load_training_data(os.getcwd() + '/training.csv')\n",
    "\n",
    "VALIDATION_SIZE = 50\n",
    "print('split them into training_set and validation_set (2140-100 : 100)')\n",
    "X_train, y_train = X[VALIDATION_SIZE:], labels[VALIDATION_SIZE:]\n",
    "X_valid, y_valid = X[:VALIDATION_SIZE], labels[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CNN network\n",
    "\n",
    "def init_W(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def init_bias(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "\n",
    "\n",
    "'''--------------  build the network model -------------- \n",
    "input:  x       96x96, 1 channel                (? * 96 * 96 * 1)\n",
    "        y_      labels of 15 keypoint(x, y)     (? * 30)\n",
    "        keep_prob  dropout coefficient\n",
    "output: \n",
    "        predict_conv    the prediction of 15    (? * 30)\n",
    "        rmse    root mean square error (loss function)\n",
    "'''\n",
    "def nn_model(x, y_, keep_prob):\n",
    "    # ------------- first convolutional layer pack --------------\n",
    "    # init conv layer. 32 kernels with size of 3x3\n",
    "    W_conv1 = init_W([3, 3, 1, 32])\n",
    "    b_conv1 = init_bias([32])\n",
    "    # convolute, relu. output shape: ? x 94x94 x 32\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    print(\"h_conv1.shape: \", h_conv1.shape)\n",
    "    # max pool 2x2. output shape: ? x 47x47 x 32\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    # ------------- second convolutional layer pack --------------\n",
    "    # init conv layer. 64 kernels with size of 3x3\n",
    "    W_conv2 = init_W([3, 3, 32, 64])\n",
    "    b_conv2 = init_bias([64])\n",
    "    # convolute, relu. output shape: ? x 45x45 x 64\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    print(\"h_conv2.shape: \", h_conv2.shape)\n",
    "    # max pool 2x2. output shape: ? x 23x23 x 64\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    # ------------- third convolutional layer pack --------------\n",
    "    # init conv layer. 128 kernels with size of 2x2\n",
    "    W_conv3 = init_W([2, 2, 64, 128])\n",
    "    b_conv3 = init_bias([128])\n",
    "    # convolute, relu. output shape: ? x 22x22 x 128\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "    print(\"h_conv3.shape: \", h_conv3.shape)\n",
    "    # max pool 2x2. output shape: ? x 11x11 x 128\n",
    "    h_pool3 = max_pool_2x2(h_conv3)\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    # ------------- fourth convolutional layer pack --------------\n",
    "    # init conv layer. 256 kernels with size of 2x2\n",
    "    W_conv4 = init_W([2, 2, 128, 256])\n",
    "    b_conv4 = init_bias([256])\n",
    "    # convolute, relu. output shape: ? x 10x10 x 256\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n",
    "    print(\"h_conv4.shape: \", h_conv4.shape)\n",
    "    # max pool 2x2. output shape: ? x 5x5 x 256\n",
    "    h_pool4 = max_pool_2x2(h_conv4)\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    # ------------- first fully connection layer --------------\n",
    "    featuremap_size = 5 * 5 * 256 # 6400\n",
    "    # flatten the featuremap from last conv_layer\n",
    "    h_pool4_flat = tf.reshape(h_pool4, [-1, featuremap_size])\n",
    "    # init fc_layer. (6400) to 512\n",
    "    W_fc1 = init_W([featuremap_size, 512])\n",
    "    b_fc1 = init_bias([512])\n",
    "    # ?x(6400) -> ?x512\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n",
    "#    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # ------------- second fully connection layer --------------\n",
    "    # init fc_layer. 512 to 512 neurons\n",
    "    W_fc2 = init_W([512, 512])\n",
    "    b_fc2 = init_bias([512])\n",
    "    # ?x512 -> ?x512\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # ------------- third fully connection layer --------------\n",
    "    # init fc_layer. 512 to 30 neurons\n",
    "    W_fc3 = init_W([512, 30])\n",
    "    b_fc3 = init_bias([30])\n",
    "    # ?x512 -> ?x30  \n",
    "    predict_conv = tf.matmul(h_fc2_drop, W_fc3) + b_fc3\n",
    "    # ---------------------------------------------------------\n",
    "    # predict_conv is the prediction of 15 keypoints (x,y)\n",
    "\n",
    "    # -------- root mean square error (loss function) --------\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(y_ - predict_conv)))\n",
    "    return predict_conv, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1200\n",
    "BATCH_SIZE = 64\n",
    "EARLY_STOP_PATIENCE = 20\n",
    "\n",
    "# ------------------------ train the model --------------------------\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# pipeline starts on placeholds \n",
    "x = tf.placeholder(\"float\", shape = [None, 96, 96, 1])\n",
    "y_ = tf.placeholder(\"float\", shape = [None, 30])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "# build network model\n",
    "predict_conv, rmse = nn_model(x, y_, keep_prob)\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(rmse)\n",
    "\n",
    "TRAIN_SIZE = X_train.shape[0]\n",
    "print('begin training..., train dataset size:{0}'.format(TRAIN_SIZE))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "best_validation_loss = 1000000.0\n",
    "current_epoch = 0\n",
    "for i in range(EPOCHS):                                     # each epoch\n",
    "    # shuffle the indices of training samples\n",
    "    TRAIN_SIZE = X_train.shape[0]\n",
    "    train_index = list(range(0, TRAIN_SIZE, 1))\n",
    "    random.shuffle(train_index)\n",
    "    X_train, y_train = X_train[train_index], y_train[train_index]\n",
    "\n",
    "    for j in tqdm(range(0, TRAIN_SIZE, BATCH_SIZE)):        # each batch\n",
    "        # print('epoch {0}, train {1} samples done...'.format(i, j))\n",
    "        train_step.run(feed_dict = {x : X_train[j : j + BATCH_SIZE], \\\n",
    "                                   y_ : y_train[j : j + BATCH_SIZE], keep_prob : 0.5})\n",
    "\n",
    "    train_loss = rmse.eval(feed_dict = {x : X_train, y_ : y_train, keep_prob : 1.0})\n",
    "    validation_loss = rmse.eval(feed_dict={x : X_valid, y_ : y_valid, keep_prob : 1.0})\n",
    "\n",
    "    print('epoch {0} done! training loss:{1}; validation loss:{2}'.format(i, train_loss*96.0, validation_loss*96.0))\n",
    "\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        current_epoch = i\n",
    "\n",
    "        # ---------- save the model ---------- \n",
    "        saver = tf.train.Saver()\n",
    "        model_saved = saver.save(sess, os.getcwd() + '/model')\n",
    "        print('model saved in :{0}'.format(model_saved))\n",
    "        # ------------------------------------            \n",
    "    elif (i - current_epoch) >= EARLY_STOP_PATIENCE:\n",
    "        print('early stopping')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_testing_data(testing_file):\n",
    "    df = pd.read_csv(testing_file)\n",
    "    df['Image'] = df['Image'].apply(lambda pixel: np.fromstring(pixel, sep=' ') / 255.0)\n",
    "    X = np.vstack(df['Image'])\n",
    "    X = X.reshape((-1, 96, 96, 1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- load test set 'test.csv' to predict key_points  ----------\n",
    "X = load_testing_data(os.getcwd() + '/test.csv')\n",
    "TEST_SIZE = X.shape[0]\n",
    "predict_output = []\n",
    "\n",
    "for j in range(0, TEST_SIZE, BATCH_SIZE):\n",
    "    y_batch = predict_conv.eval(feed_dict={x : X[j : j + BATCH_SIZE], keep_prob : 1.0})\n",
    "    predict_output.extend(y_batch)\n",
    "print('done predicting test images!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ make submission ------------------------\n",
    "keypoint_index = {\n",
    "    'left_eye_center_x':0,\n",
    "    'left_eye_center_y':1,\n",
    "    'right_eye_center_x':2,\n",
    "    'right_eye_center_y':3,\n",
    "    'left_eye_inner_corner_x':4,\n",
    "    'left_eye_inner_corner_y':5,\n",
    "    'left_eye_outer_corner_x':6,\n",
    "    'left_eye_outer_corner_y':7,\n",
    "    'right_eye_inner_corner_x':8,\n",
    "    'right_eye_inner_corner_y':9,\n",
    "    'right_eye_outer_corner_x':10,\n",
    "    'right_eye_outer_corner_y':11,\n",
    "    'left_eyebrow_inner_end_x':12,\n",
    "    'left_eyebrow_inner_end_y':13,\n",
    "    'left_eyebrow_outer_end_x':14,\n",
    "    'left_eyebrow_outer_end_y':15,\n",
    "    'right_eyebrow_inner_end_x':16,\n",
    "    'right_eyebrow_inner_end_y':17,\n",
    "    'right_eyebrow_outer_end_x':18,\n",
    "    'right_eyebrow_outer_end_y':19,\n",
    "    'nose_tip_x':20,\n",
    "    'nose_tip_y':21,\n",
    "    'mouth_left_corner_x':22,\n",
    "    'mouth_left_corner_y':23,\n",
    "    'mouth_right_corner_x':24,\n",
    "    'mouth_right_corner_y':25,\n",
    "    'mouth_center_top_lip_x':26,\n",
    "    'mouth_center_top_lip_y':27,\n",
    "    'mouth_center_bottom_lip_x':28,\n",
    "    'mouth_center_bottom_lip_y':29\n",
    "}\n",
    "\n",
    "resultfile = open('./result.csv','w')\n",
    "resultfile.write('RowId, ImageId, FeatureName, Location\\n')\n",
    "submitfile = open('./submit.csv', 'w')\n",
    "submitfile.write('RowId,Location\\n')\n",
    "\n",
    "IdLookupTable = open('IdLookupTable.csv')\n",
    "IdLookupTable.readline()\n",
    "\n",
    "for line in IdLookupTable:\n",
    "    RowId,ImageId,FeatureName = line.rstrip().split(',')\n",
    "    image_index = int(ImageId) - 1\n",
    "    feature_index = keypoint_index[FeatureName]\n",
    "    feature_location = predict_output[image_index][feature_index] * 96\n",
    "    resultfile.write('{0},{1},{2},{3}\\n'.format(RowId, ImageId, FeatureName, feature_location))\n",
    "    submitfile.write('{0},{1}\\n'.format(RowId, feature_location))\n",
    "\n",
    "resultfile.close()\n",
    "submitfile.close()\n",
    "IdLookupTable.close()\n",
    "\n",
    "print('result.csv: ')\n",
    "df = pd.read_csv('./result.csv')\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
